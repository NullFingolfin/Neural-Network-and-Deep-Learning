{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da309c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#导入cart相关模块\n",
    "#决策树节点，基础二叉决策树，CART分类树和回归树\n",
    "from cart import TreeNode, BinaryDecisionTree, ClassificationTree, RegressionTree\n",
    "#数据划分模块\n",
    "from sklearn.model_selection import train_test_split\n",
    "#均方误差评估模块\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from utils import feature_split, calculate_gini, data_shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47fd850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 定义回归树的平方损失\n",
    "class SquareLoss():\n",
    "    # 定义平方损失\n",
    "    def loss(self, y, y_pred):\n",
    "        return 0.5 * np.power((y - y_pred), 2)  #（y-y_pred)^2\n",
    "    # 定义平方损失的梯度\n",
    "    def gradient(self, y, y_pred):\n",
    "        return -(y - y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c53b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GBDT定义\n",
    "class GBDT(object):\n",
    "    def __init__(self, n_estimators, learning_rate, min_samples_split,\n",
    "                 min_gini_impurity, max_depth, regression):\n",
    "        ### 常用超参数\n",
    "        # 树的棵树\n",
    "        self.n_estimators = n_estimators\n",
    "        # 学习率\n",
    "        self.learning_rate = learning_rate\n",
    "        # 结点最小分裂样本数\n",
    "        self.min_samples_split = min_samples_split\n",
    "        # 结点最小基尼不纯度\n",
    "        self.min_gini_impurity = min_gini_impurity\n",
    "        # 最大深度\n",
    "        self.max_depth = max_depth\n",
    "        # 默认为回归树\n",
    "        self.regression = regression\n",
    "        # 损失为平方损失\n",
    "        self.loss = SquareLoss()\n",
    "        # 如果是分类树，需要定义分类树损失函数\n",
    "        # 这里省略，如需使用，需自定义分类损失函数\n",
    "        if not self.regression:\n",
    "            self.loss = None\n",
    "        # 多棵树叠加\n",
    "        self.estimators = []\n",
    "        for i in range(self.n_estimators):\n",
    "            self.estimators.append(RegressionTree(min_samples_split=self.min_samples_split,\n",
    "                                             min_gini_impurity=self.min_gini_impurity,\n",
    "                                             max_depth=self.max_depth))\n",
    "    # 拟合方法\n",
    "    def fit(self, X, y):\n",
    "        # 前向分步模型初始化，第一棵树\n",
    "        self.estimators[0].fit(X, y)\n",
    "        # 第一棵树的预测结果\n",
    "        y_pred = self.estimators[0].predict(X) #此predict是来自于二叉树的predict ？\n",
    "        # 前向分步迭代训练\n",
    "        for i in range(1, self.n_estimators):\n",
    "            gradient = self.loss.gradient(y, y_pred)\n",
    "            self.estimators[i].fit(X, gradient) #用损失函数的负梯度在当前模型的值作为回归提升树中残差的近似值\n",
    "            y_pred -= np.multiply(self.learning_rate, self.estimators[i].predict(X))\n",
    "            \n",
    "    # 预测方法\n",
    "    def predict(self, X):\n",
    "        # 回归树预测\n",
    "        y_pred = self.estimators[0].predict(X)\n",
    "        for i in range(1, self.n_estimators):\n",
    "            y_pred -= np.multiply(self.learning_rate, self.estimators[i].predict(X))\n",
    "        # 分类树预测\n",
    "        if not self.regression:\n",
    "            # 将预测值转化为概率\n",
    "            y_pred = np.exp(y_pred) / np.expand_dims(np.sum(np.exp(y_pred), axis=1), axis=1)\n",
    "            # 转化为预测标签\n",
    "            y_pred = np.argmax(y_pred, axis=1) #取出最大值对应的索引 axis=1 在行方向上找最值\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf02c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GBDT分类树\n",
    "class GBDTClassifier(GBDT):\n",
    "      def __init__(self, n_estimators=200, learning_rate=.5, min_samples_split=2,\n",
    "                 min_info_gain=1e-6, max_depth=2):\n",
    "            super(GBDTClassifier, self).__init__(n_estimators=n_estimators,\n",
    "                                             learning_rate=learning_rate,\n",
    "                                             min_samples_split=min_samples_split,\n",
    "                                             min_gini_impurity=min_info_gain,\n",
    "                                             max_depth=max_depth,\n",
    "                                             regression=False)\n",
    "      # 拟合方法\n",
    "      def fit(self, X, y):\n",
    "            super(GBDTClassifier, self).fit(X, y)\n",
    "        \n",
    "### GBDT回归树\n",
    "class GBDTRegressor(GBDT):\n",
    "      def __init__(self, n_estimators=300, learning_rate=0.1, min_samples_split=2,\n",
    "                 min_var_reduction=1e-6, max_depth=3):\n",
    "        super(GBDTRegressor, self).__init__(n_estimators=n_estimators,\n",
    "                                            learning_rate=learning_rate,\n",
    "                                            min_samples_split=min_samples_split,\n",
    "                                            min_gini_impurity=min_var_reduction,\n",
    "                                            max_depth=max_depth,\n",
    "                                            regression=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f90c701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/heshuaichen/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Users/heshuaichen/numpy/机器学习—公式推导与代码实现/10—16Ensemble_Learning_监督集成/utils.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([X_left, X_right])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m GBDTRegressor()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 模型训练\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 模型预测\u001b[39;00m\n\u001b[1;32m     17\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mGBDT.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators):\n\u001b[1;32m     38\u001b[0m     gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mgradient(y, y_pred)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#用损失函数的负梯度在当前模型的值作为回归提升树中残差的近似值\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     y_pred \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmultiply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators[i]\u001b[38;5;241m.\u001b[39mpredict(X))\n",
      "File \u001b[0;32m~/numpy/机器学习—公式推导与代码实现/10—16Ensemble_Learning_监督集成/cart.py:183\u001b[0m, in \u001b[0;36mRegressionTree.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimpurity_calculation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_variance_reduction\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_leaf_value_calculation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mean_of_y\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRegressionTree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/numpy/机器学习—公式推导与代码实现/10—16Ensemble_Learning_监督集成/cart.py:43\u001b[0m, in \u001b[0;36mBinaryDecisionTree.fit\u001b[0;34m(self, X, y, loss)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# 递归构建决策树\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/numpy/机器学习—公式推导与代码实现/10—16Ensemble_Learning_监督集成/cart.py:75\u001b[0m, in \u001b[0;36mBinaryDecisionTree._build_tree\u001b[0;34m(self, X, y, current_depth)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# 遍历取值并寻找最佳特征分裂阈值\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m threshold \u001b[38;5;129;01min\u001b[39;00m unique_values:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# 特征节点二叉分裂\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     Xy1, Xy2 \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# 如果分裂后的子集大小都不为0\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(Xy1) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(Xy2) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;66;03m# 获取两个子集的标签值\u001b[39;00m\n",
      "File \u001b[0;32m~/numpy/机器学习—公式推导与代码实现/10—16Ensemble_Learning_监督集成/utils.py:12\u001b[0m, in \u001b[0;36mfeature_split\u001b[0;34m(X, feature_i, threshold)\u001b[0m\n\u001b[1;32m      9\u001b[0m     split_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m sample: sample[feature_i] \u001b[38;5;241m==\u001b[39m threshold\n\u001b[1;32m     11\u001b[0m X_left \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([sample \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m X \u001b[38;5;28;01mif\u001b[39;00m split_func(sample)])\n\u001b[0;32m---> 12\u001b[0m X_right \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([sample \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m X \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m split_func(sample)])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([X_left, X_right])\n",
      "File \u001b[0;32m~/numpy/机器学习—公式推导与代码实现/10—16Ensemble_Learning_监督集成/utils.py:12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m     split_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m sample: sample[feature_i] \u001b[38;5;241m==\u001b[39m threshold\n\u001b[1;32m     11\u001b[0m X_left \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([sample \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m X \u001b[38;5;28;01mif\u001b[39;00m split_func(sample)])\n\u001b[0;32m---> 12\u001b[0m X_right \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([sample \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m X \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43msplit_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([X_left, X_right])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### GBDT分类树\n",
    "# 导入sklearn数据集模块\n",
    "from sklearn import datasets\n",
    "# 导入波士顿房价数据集\n",
    "boston = datasets.load_boston()\n",
    "# 打乱数据集\n",
    "X, y = data_shuffle(boston.data, boston.target, seed=13)\n",
    "X = X.astype(np.float32)\n",
    "offset = int(X.shape[0] * 0.9)\n",
    "# 划分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "# 创建GBRT实例\n",
    "model = GBDTRegressor()\n",
    "# 模型训练\n",
    "model.fit(X_train, y_train)\n",
    "# 模型预测\n",
    "y_pred = model.predict(X_test)\n",
    "# 计算模型预测的均方误差\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print (\"Mean Squared Error of NumPy GBRT:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9db18276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error of sklearn GBRT: 14.88424955427429\n"
     ]
    }
   ],
   "source": [
    "# 导入sklearn GBDT模块\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# 创建模型实例\n",
    "reg = GradientBoostingRegressor(n_estimators=200, learning_rate=0.5,\n",
    "                                 max_depth=4, random_state=0)\n",
    "# 模型拟合\n",
    "reg.fit(X_train, y_train)\n",
    "# 模型预测\n",
    "y_pred = reg.predict(X_test)\n",
    "# 计算模型预测的均方误差\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print (\"Mean Squared Error of sklearn GBRT:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c88022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
